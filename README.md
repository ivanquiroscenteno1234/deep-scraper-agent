# ðŸ•µï¸ Deep Scraper Agent

Autonomous web navigation, search, and data extraction powered by **LangGraph + Gemini**.

## Overview

Deep Scraper Agent is an AI-powered web scraping tool that can autonomously navigate websites, handle disclaimers, fill search forms, and extract structured data from results pages. It uses Google's Gemini AI models for decision-making and Playwright for browser automation via MCP (Model Context Protocol).

## Features

- ðŸ¤– **Autonomous Navigation**: AI-driven page analysis and decision making
- ðŸ” **Smart Search**: Automatically identifies and fills search forms
- ðŸ“Š **Data Extraction**: Extracts tabular data and generates Playwright scripts
- ðŸŽ¯ **Disclaimer Handling**: Automatically clicks accept/agree buttons
- ðŸ”„ **Self-Correcting**: LLM-powered script fixing loop
- ðŸŒ **Modern UI**: React frontend with real-time WebSocket logs
- âš¡ **FastAPI Backend**: High-performance async API

---

## ðŸš€ Quick Start (Run the App)

You need **3 terminals** to run the full application:

### Terminal 1: MCP Server (Playwright)
```bash
cd "e:\Ivan (IMPORTANTE)\Ivan\Disco D\Ideas\Script Builder"
npx @executeautomation/playwright-mcp-server --port 8931
```

### Terminal 2: Backend API (FastAPI)
```bash
cd "e:\Ivan (IMPORTANTE)\Ivan\Disco D\Ideas\Script Builder\backend"
python main.py
```

### Terminal 3: Frontend (React + Vite)
```bash
cd "e:\Ivan (IMPORTANTE)\Ivan\Disco D\Ideas\Script Builder\frontend"
npm run dev
```

### Access the App
Open your browser and go to: **http://localhost:5173/**

---

## ðŸ“‹ Services Summary

| Service | Port | URL | Command |
|---------|------|-----|---------|
| Frontend | 5173 | http://localhost:5173/ | `npm run dev` |
| Backend API | 8000 | http://localhost:8000/ | `python main.py` |
| MCP Server | 8931 | http://localhost:8931/ | `npx @executeautomation/playwright-mcp-server --port 8931` |

---

## Project Structure

```
deep-scraper-agent/
â”œâ”€â”€ frontend/                 # React + Vite frontend
â”‚   â”œâ”€â”€ src/                  # React components
â”‚   â”œâ”€â”€ package.json          # Node dependencies
â”‚   â””â”€â”€ vite.config.ts        # Vite configuration
â”‚
â”œâ”€â”€ backend/                  # FastAPI backend
â”‚   â”œâ”€â”€ main.py               # API entry point
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ output/               # Generated scripts output
â”‚
â”œâ”€â”€ deep_scraper/             # Core scraping engine
â”‚   â”œâ”€â”€ core/                 # State, browser, schemas
â”‚   â”œâ”€â”€ graph/                # LangGraph workflow & nodes
â”‚   â”œâ”€â”€ utils/                # DOM helpers, prompts
â”‚   â””â”€â”€ compiler/             # Script generation
â”‚
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ requirements.txt          # Root Python dependencies
â””â”€â”€ output/                   # Output files & data
```

---

## Installation (First-Time Setup)

1. Clone the repository:
```bash
git clone https://github.com/ivanquiroscenteno1234/deep-scraper-agent.git
cd deep-scraper-agent
```

2. Create a Python virtual environment:
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac
```

3. Install Python dependencies (root + backend):
```bash
pip install -r requirements.txt
pip install -r backend/requirements.txt
```

4. Install Playwright browsers:
```bash
playwright install chromium
```

5. Install Node.js dependencies (frontend):
```bash
cd frontend
npm install
cd ..
```

6. Set up your environment variables:
```bash
# Create .env file with your Google API key
echo GOOGLE_API_KEY=your_api_key_here > .env
```

---

## Prerequisites

Make sure you have installed:

- **Python 3.10+**: [Download Python](https://www.python.org/downloads/)
- **Node.js 18+**: [Download Node.js](https://nodejs.org/)
- **npm** (comes with Node.js)
- **Google API Key**: For Gemini AI models

---

## How It Works

1. **Navigate**: Agent navigates to the target URL via MCP-controlled browser
2. **Analyze**: LLM analyzes page content to determine page type
3. **Click/Accept**: If on a disclaimer page, clicks the accept button
4. **Search**: Fills the search form with the query and submits
5. **Capture**: Records column mappings and grid selectors
6. **Generate**: LLM generates a complete Playwright Python script
7. **Test & Fix**: Executes script and uses LLM to fix any errors

---

## Configuration

Environment variables (`.env`):
- `GOOGLE_API_KEY`: Your Google AI API key (required)
- `GEMINI_MODEL`: Model to use (default: `gemini-3-flash-preview`)

---

## Troubleshooting

### Port already in use
If a port is already in use, kill the process or use a different port:
```bash
# Check what's using a port (Windows)
netstat -ano | findstr :8931

# Kill by PID
taskkill /PID <pid> /F
```

### MCP Server not connecting
Ensure the MCP server is running before starting the backend:
```bash
npx @executeautomation/playwright-mcp-server --port 8931
```

### Frontend not loading
Make sure dependencies are installed:
```bash
cd frontend
npm install
npm run dev
```

---

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/run-agent` | POST | Start the scraper agent |
| `/api/execute-script` | POST | Execute a generated script |
| `/ws` | WebSocket | Real-time log streaming |
| `/health` | GET | Health check |

---

## ðŸ”® Future Implementations

Ideas and integrations to explore in future versions:

- **[Antigravity Kit](https://github.com/vudovn/antigravity-kit)** - A AI kit that could enhance the AI Experience with modern agents and skills.

---