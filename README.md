# ðŸ•µï¸ Deep Scraper Agent

Autonomous web navigation, search, and data extraction powered by **LangGraph + Gemini**.

## Overview

Deep Scraper Agent is an AI-powered web scraping tool that can autonomously navigate websites, handle disclaimers, fill search forms, and extract structured data from results pages. It uses Google's Gemini AI models for decision-making and Playwright for browser automation.

## Features

- ðŸ¤– **Autonomous Navigation**: AI-driven page analysis and decision making
- ðŸ” **Smart Search**: Automatically identifies and fills search forms
- ðŸ“Š **Data Extraction**: Extracts tabular data and saves to CSV
- ðŸŽ¯ **Disclaimer Handling**: Automatically clicks accept/agree buttons
- ðŸ”„ **Self-Correcting**: Fallback strategies for robust element interaction

## Project Structure

```
deep-scraper-agent/
â”œâ”€â”€ app.py                    # Streamlit UI entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env                      # Environment variables (GOOGLE_API_KEY)
â”‚
â”œâ”€â”€ deep_scraper/             # Main package
â”‚   â”œâ”€â”€ core/                 # Core components
â”‚   â”‚   â”œâ”€â”€ state.py          # Agent state definition
â”‚   â”‚   â”œâ”€â”€ browser.py        # Playwright browser manager
â”‚   â”‚   â””â”€â”€ schemas.py        # Pydantic models for LLM output
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                # LangGraph engine
â”‚   â”‚   â”œâ”€â”€ engine.py         # Graph workflow definition
â”‚   â”‚   â””â”€â”€ nodes.py          # Node implementations
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/               # Alternative agent implementations
â”‚   â”‚   â”œâ”€â”€ explorer.py       # Tool-calling explorer agent
â”‚   â”‚   â””â”€â”€ visual.py         # Visual/screenshot-based agent
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                # Utilities
â”‚   â”‚   â”œâ”€â”€ dom.py            # DOM simplification helpers
â”‚   â”‚   â””â”€â”€ prompts.py        # LLM system prompts
â”‚   â”‚
â”‚   â””â”€â”€ compiler/             # Script generation
â”‚       â”œâ”€â”€ compiler.py       # Converts steps to Playwright scripts
â”‚       â””â”€â”€ template.py       # Script templates
â”‚
â”œâ”€â”€ output/                   # Output files
â”‚   â””â”€â”€ extracted_data/       # Saved CSV results
â”‚
â””â”€â”€ tests/                    # Test suite
```

## Installation

1. Clone the repository:
```bash
git clone https://github.com/ivanquiroscenteno1234/deep-scraper-agent.git
cd deep-scraper-agent
```

2. Create a virtual environment:
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac
```

3. Install dependencies:
```bash
pip install -r requirements.txt
playwright install chromium
```

4. Set up your environment:
```bash
# Create .env file with your Google API key
echo GOOGLE_API_KEY=your_api_key_here > .env
```

## Usage

### Running the Streamlit App

```bash
streamlit run app.py
```

This launches a web UI where you can:
1. Enter a target URL (e.g., a county clerk search page)
2. Enter a search term (e.g., a name to search for)
3. Click "Launch Agent" to start the autonomous scraping

### Running the Graph Engine Directly

```python
import asyncio
from deep_scraper.graph.engine import app
from deep_scraper.core.state import AgentState
from deep_scraper.core.browser import BrowserManager

async def main():
    initial_state = AgentState(
        target_url="https://example.com/search",
        search_query="John Smith",
        current_page_summary="",
        logs=[],
        attempt_count=0,
        status="NAVIGATING",
        extracted_data=[],
        search_selectors={}
    )
    
    async for output in app.astream(initial_state):
        print(output)
    
    # Cleanup
    browser = BrowserManager()
    await browser.close()

asyncio.run(main())
```

## How It Works

1. **Navigate**: Agent navigates to the target URL
2. **Analyze**: LLM analyzes page content to determine if it's the search page
3. **Click/Accept**: If on a disclaimer page, clicks the accept button
4. **Search**: Fills the search form with the query and submits
5. **Extract**: Parses the results table and saves data to CSV

## Configuration

Environment variables (`.env`):
- `GOOGLE_API_KEY`: Your Google AI API key (required)
- `GEMINI_MODEL`: Model to use (default: `gemini-2.0-flash-exp`)

## License

MIT License
