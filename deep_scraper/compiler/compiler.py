"""
Script Compiler - Generates Playwright scripts from recorded steps.

Compiles recorded MCP steps into a standalone Python script that can
navigate and extract data from county clerk websites.

See .agent/workflows/project-specification.md for workflow details.
"""

import json
from typing import Dict, List, Optional


def compile_steps_to_script(
    steps: List[Dict],
    county_name: str,
    search_variable: str = None,
    column_mapping: Dict[str, str] = None,
    grid_selector: str = None
) -> str:
    """
    Compile recorded steps into a Playwright script.
    
    Args:
        steps: List of recorded steps from MCP codegen
        county_name: Name of the county (for function naming)
        search_variable: Search term to replace with variable
        column_mapping: Map of column names to use for extraction
        grid_selector: CSS selector for the results grid
        
    Returns:
        Complete Python script as a string
    """
    
    grid_selector = grid_selector or "#RsltsGrid table"
    column_mapping = column_mapping or {}
    
    script_body = ""
    
    for step in steps:
        # MCP steps use 'action', old steps use 'action_type'
        action = step.get('action') or step.get('action_type', '')
        selector = step.get('selector', '')
        value = step.get('value', '')
        url = step.get('url', '')
        description = step.get('description', '')
        
        if description:
            script_body += f"\n        # {description}\n"
        
        if action in ('goto', 'navigate'):
            target_url = url or value
            script_body += f'        page.goto("{target_url}")\n'
            script_body += '        page.wait_for_load_state("networkidle")\n'
            
        elif action == 'click':
            script_body += f'        page.wait_for_selector("{selector}", timeout=5000)\n'
            script_body += f'        page.click("{selector}")\n'
            script_body += '        page.wait_for_load_state("networkidle")\n'
            
        elif action == 'fill':
            script_body += f'        page.wait_for_selector("{selector}", timeout=5000)\n'
            
            # Replace {{SEARCH_TERM}} placeholder or specific search value
            if value == "{{SEARCH_TERM}}":
                script_body += f'        page.fill("{selector}", search_term)\n'
            elif search_variable and value and search_variable.lower() in value.lower():
                script_body += f'        page.fill("{selector}", search_term)\n'
                script_body += f'        # Replaced "{value}" with variable\n'
            else:
                script_body += f'        page.fill("{selector}", "{value}")\n'
                
        elif action == 'wait':
            script_body += '        page.wait_for_timeout(2000)\n'
            
        elif action == 'capture_grid':
            # Skip meta-step, handled by extraction logic below
            pass

    # Build column list for headers
    column_list = list(column_mapping.values()) if column_mapping else []
    columns_json = json.dumps(column_list) if column_list else '[]'
    
    # Generate function name from county
    func_name = county_name.replace(' ', '_').replace('-', '_').lower()
    
    template = f'''"""
Playwright script for {county_name} county clerk records.
Generated by Deep Scraper Agent.

Usage:
    python {func_name}_scraper.py "Search Term"
"""

from playwright.sync_api import sync_playwright
import pandas as pd
import datetime
import sys
import json


def scrape_{func_name}(search_term: str) -> str:
    """
    Scrape {county_name} records for a given search term.
    
    Args:
        search_term: Name to search for
        
    Returns:
        Path to CSV file with results
    """
    results = []
    expected_columns = {columns_json}
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()
        page.set_default_timeout(10000)
        
        try:
{script_body}
            
            # --- Wait for results grid ---
            print("Waiting for results grid...")
            page.wait_for_selector("{grid_selector}", timeout=15000)
            page.wait_for_timeout(2000)  # Allow full render
            
            # --- Extract data from grid ---
            print("Extracting data...")
            
            # Find the grid
            grid = page.locator("{grid_selector}")
            
            # Get headers
            headers = []
            header_cells = grid.locator("thead th, tr:first-child th")
            if header_cells.count() > 0:
                for i in range(header_cells.count()):
                    headers.append(header_cells.nth(i).inner_text().strip())
            
            # Use expected columns if no headers found
            if not headers and expected_columns:
                headers = expected_columns
            
            # Get data rows
            rows = grid.locator("tbody tr")
            row_count = rows.count()
            print(f"Found {{row_count}} rows")
            
            for i in range(row_count):
                row = rows.nth(i)
                cells = row.locator("td")
                
                if cells.count() > 0:
                    row_data = {{}}
                    for j in range(cells.count()):
                        header = headers[j] if j < len(headers) else f"col_{{j}}"
                        row_data[header] = cells.nth(j).inner_text().strip()
                    results.append(row_data)
            
            # Save results
            if results:
                df = pd.DataFrame(results)
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_name = "".join([c for c in search_term if c.isalnum() or c in (' ', '_')]).strip().replace(' ', '_')
                filename = f"results_{{safe_name}}_{{timestamp}}.csv"
                df.to_csv(filename, index=False)
                print(f"Saved {{len(results)}} records to {{filename}}")
                
                # Also save as JSON
                json_filename = filename.replace('.csv', '.json')
                with open(json_filename, 'w') as f:
                    json.dump(results, f, indent=2)
                
                return filename
            else:
                print("No results found.")
                return None
                
        except Exception as e:
            print(f"Error: {{e}}")
            page.screenshot(path="error_screenshot.png")
            raise e
        finally:
            browser.close()


if __name__ == "__main__":
    if len(sys.argv) > 1:
        search = sys.argv[1]
    else:
        search = "{search_variable or 'Test Name'}"
    
    result = scrape_{func_name}(search)
    if result:
        print(f"Done! Results saved to: {{result}}")
'''
    return template
